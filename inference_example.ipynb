{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/newdisk/jxh/anaconda/envs/sft2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "The model is automatically converting to bf16 for faster inference. If you want to disable the automatic precision, please manually add bf16/fp16/fp32=True to \"AutoModelForCausalLM.from_pretrained\".\n",
      "/home/jxh/.cache/huggingface/modules/transformers_modules/Qwen-7B-Chat/modeling_qwen.py:314: UserWarning: KV cache kernel source files (.cpp and .cu) not found.\n",
      "  warnings.warn(\"KV cache kernel source files (.cpp and .cu) not found.\")\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.95it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,2\"\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import  AutoTokenizer,AutoModelForCausalLM, GenerationConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "model_id = \"models/Base/Qwen-7B-Chat\"\n",
    "model_id = \"models/Lora/Qwen_7B_Chat_lora_step1\"\n",
    "model_id = \"models/Lora/Qwen_7B_Chat_lora_step2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    load_in_4bit = True,\n",
    "    use_cache_quantization=True,\n",
    "    use_cache_kernel=True, \n",
    "    trust_remote_code=True,\n",
    "    use_flash_attn = False,\n",
    ").eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_fast=False, \n",
    "    trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"chat_format\": \"chatml\",\n",
       "  \"do_sample\": true,\n",
       "  \"eos_token_id\": 151643,\n",
       "  \"max_new_tokens\": 256,\n",
       "  \"max_window_size\": 6144,\n",
       "  \"pad_token_id\": 151643,\n",
       "  \"repetition_penalty\": 1.1,\n",
       "  \"temperature\": 0.4,\n",
       "  \"top_k\": 0,\n",
       "  \"top_p\": 0.7\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config.temperature = 0.4\n",
    "model.generation_config.max_new_tokens = 256\n",
    "model.generation_config.top_p = 0.7\n",
    "\n",
    "model.generation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该文本情感是中性的。\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "     \"ST星源2022年12月2日在深交所互动易中披露，截至2022年11月30日公司股东户数为8.99万户，较上期（2022年9月30日）减少447户，减幅为0.49%。ST星源股东户数高于行业平均水平。根据Choice数据，截至2022年11月30日环保行业上市公司平均股东户数为2.95万户。其中，公司股东户数处于0.5万~1.5万区间占比最高，为26.40%。环保行业股东户数分布股东户数与股价2022年6月30日至今，公司股东户数有所下降，区间跌幅为3.17%。2022年6月30日至2022年11月30日区间股价上涨3.85%。股东户数及股价股东户数与股本截至2022年11月30日，公司最新总股本为10.59亿股，其中流通股本为10.58亿股。户均持有流通股数量由上期的1.17万股上升至1.18万股，户均流通市值2.22万元。户均持股金额\",\n",
    "     \"平安银行总经理内部讲话公开，要相信公司前景\",\n",
    "]\n",
    "\n",
    "system_prompt = \"你是一个中文文本分类器，判断以下金融文本的情绪类别，答案为积极、消极或者中性。\"\n",
    "\n",
    "response, history = model.chat(tokenizer, data[0], history = None, system = system_prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "显性\n"
     ]
    }
   ],
   "source": [
    "model.disable_adapters()\n",
    "system_prompt = \"你是一个中文文本分类器，判断以下金融文本表达情绪的显隐性，答案为显性或隐性。直抒胸臆，直接表达看法为主则为显性；通过数据来支持或暗示观点为隐性。\"\n",
    "\n",
    "response, history = model.chat(tokenizer, data[1], history = None, system = system_prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
